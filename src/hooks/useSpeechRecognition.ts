
import { useState, useRef, useCallback } from 'react';

export const useSpeechRecognition = () => {
  const [isListening, setIsListening] = useState(false);
  const [transcript, setTranscript] = useState('');
  const recognitionRef = useRef<SpeechRecognition | null>(null);

  const startListening = useCallback(() => {
    console.log('ğŸ¤ COMPREHENSIVE DEBUG: Attempting to start speech recognition...');
    console.log('ğŸ” Browser check: webkitSpeechRecognition:', 'webkitSpeechRecognition' in window);
    console.log('ğŸ” Browser check: SpeechRecognition:', 'SpeechRecognition' in window);
    console.log('ğŸ” Current isListening state:', isListening);
    console.log('ğŸ” Current transcript state:', transcript);
    
    // Check browser support with detailed logging
    if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
      console.error('âŒ Speech recognition not supported in this browser');
      console.error('âŒ User Agent:', navigator.userAgent);
      alert('Speech recognition is not supported in your browser. Please use Chrome or Safari.');
      return;
    }

    // Check microphone permissions
    navigator.mediaDevices?.getUserMedia({ audio: true })
      .then(() => {
        console.log('âœ… Microphone access granted');
      })
      .catch((error) => {
        console.error('âŒ Microphone access denied:', error);
        alert('Microphone access is required for speech recognition. Please allow microphone access.');
        return;
      });

    // Stop any existing recognition before starting a new one
    if (recognitionRef.current) {
      console.log('ğŸ›‘ Stopping existing recognition before starting new one');
      recognitionRef.current.stop();
      recognitionRef.current = null;
    }

    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    recognitionRef.current = new SpeechRecognition();
    
    if (recognitionRef.current) {
      console.log('ğŸ”§ Configuring speech recognition...');
      recognitionRef.current.continuous = false;
      recognitionRef.current.interimResults = true; // Enable interim results for better debugging
      recognitionRef.current.lang = 'ar-SA'; // Arabic language
      recognitionRef.current.maxAlternatives = 3; // Get multiple alternatives
      
      recognitionRef.current.onstart = () => {
        console.log('âœ… SPEECH RECOGNITION STARTED SUCCESSFULLY');
        console.log('âœ… Setting isListening to true');
        setIsListening(true);
      };
      
      recognitionRef.current.onresult = (event) => {
        console.log('ğŸ™ï¸ SPEECH RECOGNITION RESULT EVENT TRIGGERED');
        console.log('ğŸ™ï¸ Event:', event);
        console.log('ğŸ™ï¸ Results length:', event.results.length);
        
        if (event.results.length > 0) {
          const result = event.results[0][0].transcript;
          const confidence = event.results[0][0].confidence;
          const isFinal = event.results[0].isFinal;
          
          console.log('ğŸ™ï¸ Speech result:', result);
          console.log('ğŸ“Š Confidence:', confidence);
          console.log('ğŸ Is final:', isFinal);
          
          if (isFinal) {
            console.log('âœ… FINAL RESULT - Setting transcript:', result);
            setTranscript(result);
          } else {
            console.log('â³ Interim result:', result);
          }
          
          // Log all alternatives
          for (let i = 0; i < event.results[0].length; i++) {
            console.log(`ğŸ™ï¸ Alternative ${i}:`, event.results[0][i].transcript, 'confidence:', event.results[0][i].confidence);
          }
        }
      };
      
      recognitionRef.current.onend = () => {
        console.log('ğŸ›‘ SPEECH RECOGNITION ENDED');
        console.log('ğŸ›‘ Setting isListening to false');
        setIsListening(false);
      };
      
      recognitionRef.current.onerror = (event) => {
        console.error('âŒ SPEECH RECOGNITION ERROR:', event.error);
        console.error('âŒ Error details:', event);
        console.error('âŒ Error message:', event.message);
        console.error('âŒ Setting isListening to false');
        setIsListening(false);
        
        // Provide user-friendly error messages
        switch (event.error) {
          case 'no-speech':
            console.warn('âš ï¸ No speech was detected');
            break;
          case 'audio-capture':
            console.error('âŒ Audio capture failed - microphone issues');
            alert('Microphone access failed. Please check your microphone and permissions.');
            break;
          case 'not-allowed':
            console.error('âŒ Speech recognition not allowed');
            alert('Speech recognition permission denied. Please allow microphone access.');
            break;
          case 'network':
            console.error('âŒ Network error during speech recognition');
            break;
          default:
            console.error('âŒ Unknown speech recognition error:', event.error);
        }
      };
      
      try {
        console.log('ğŸš€ CALLING recognition.start()...');
        recognitionRef.current.start();
        console.log('ğŸš€ recognition.start() called successfully');
      } catch (error) {
        console.error('âŒ EXCEPTION when starting speech recognition:', error);
        setIsListening(false);
      }
    } else {
      console.error('âŒ Failed to create SpeechRecognition instance');
    }
  }, []);

  const stopListening = useCallback(() => {
    if (recognitionRef.current) {
      recognitionRef.current.stop();
    }
    setIsListening(false);
  }, []);

  const resetTranscript = useCallback(() => {
    console.log('ğŸ§¹ Resetting transcript from:', transcript, 'to empty');
    setTranscript('');
  }, [transcript]);

  return {
    isListening,
    transcript,
    startListening,
    stopListening,
    resetTranscript
  };
};
